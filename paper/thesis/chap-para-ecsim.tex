\chapter{Parareal Solvers for Particle-in-Cell Simulations}
\label{cha: methods and results}

%%% ============================================================================================ %%%

We now investigate the performance of the parareal method applied to particle-in-cell (PIC) simulations using a simplified model of the energy-conserving semi-implicit method (ECSIM). This simplified model only takes one dimension into account for the spatial discretisation. However, the velocity and field vectors at each position are tracked in either one dimension (1D1V) or three dimensions (1D3V). This enables the simulation of complex phenomena while reducing computational complexity.

Firstly, the efficacy of parareal is tested using a well-known implicit method: the Crank--Nicolson (CN) integrator. We apply this method to the one-dimensional diffusion equation and show that CN indeed achieves second-order accurate solutions, with and without parareal.

The performance of the ECSIM solver with parareal is the main point of interest for this project. To test the validity of the code implementation of the ECSIM algorithm, unit tests are performed and we compare our simulation results with those presented by Lapenta in 2017 and 2023 \cite{lapenta_exactly_2017,lapenta_advances_2023}. 
 The efficacy of the parareal algorithm applied to a method is quantified using the following metrics:
 \begin{itemize}
    
    \item \textit{Accuracy}, calculated as the error compared to the serial case, i.e., ECSIM without parareal
    
    \item \textit{Speedup}, defined as the ratio of time taken by the serial solve to that of the parareal solve $\left(\frac{T_\mathrm{serial}}{T_\mathrm{parareal}}\right)$
    
    \item \textit{Parallel efficiency}, defined as the ratio of the speedup obtained to the number of cores used $\left(\frac{\text{speedup}}{\text{number of cores}}\right)$
    
    \item \textit{Computational runtime}, the simulation time elapsed
 
 \end{itemize}
We analyse these aspects of the parareal algorithm using the following set of test cases:

\begin{itemize}

    \item CASE I: Diffusion equation

    \item CASE II: Smooth, periodic initial plasma conditions

    \item CASE III: Two-stream instability, an electrostatic toy problem

    \item CASE IV: Weibel or transverse stream instability, an electromagnetic toy problem
    \end{itemize}
 We further perform parameter studies to investigate the behaviour of the parareal algorithm for PIC simulations. The following parameter studies are considered: 
 \begin{itemize}
 
    \item Temporal coarsening
    
    \item Scaling tests on shared-memory architecture
    
    \item Different linear solvers (used during ECSIM)
 
 \end{itemize}
A spatial coarsening test was also considered (Appendix \ref{app: spatial coarsening}). However, elementary tests revealed that studying the efficacy of spatial coarsening would require a comprehensive analysis with various high-order polynomial interpolation methods, which is beyond this project's scope. 

Note that parareal solvers are known to exhibit poor conservation of energy \cite{gander_analysis_2014}. To assess the correctness of the implementation, energy conservation is shown during serial solutions. We do not concern ourselves with the \textit{exact} energy-conserving property of ECSIM when using parareal. However, the error incurred on the energy should be bounded, which our results show can be achieved.

% A convergence test will prove that the Parareal algorithm approximates the fine solution up to a desired tolerance. Another test will investigate the different possible implementations of the tolerance. The tolerance should define at which point a solution is converged. This can be implemented in two ways: one can check whether the state change across all time steps is below a certain tolerance, or one can look at each timestep separately. The second implementation allows for fewer calculations since the number of timesteps that still need to be calculated at the later Parareal iterations should be smaller. This method might also inhibit the algorithm's convergence, leading to longer simulation times.


% \pjd{Since you don't get exact energy conservation, phrases like "Any implementation must thus have this property." or "These test cases must show that energy is conserved" would destroy the credibility of your work. The description of the test cases can go in the respective subsections where you perform the tests. Other than that, I suggest that you remove this para.}

% As stated in \ref{subsec: plasma intro ECSIM}, ECSIM is a particle-in-cell method developed in \cite{lapenta_exactly_2017} to be a direct PIC solver which is fully energy conserving. Any implementation must thus have this property. The energy conservation is calculated as a relative error of the energy at a timepoint $t$, $E(t)$, and the initial energy, $E(0)$: $\frac{|E(t) - E(0)|}{E(0)}$. 
% Two chaotic test problems will be used to assess the correctness of the implementation: an electrostatic two-stream instability and an electromagnetic Weibel instability. An initial test is also performed by manually setting the fields to zero, and the particle mover will perfectly conserve energy based on the initial momentum; this experiment will be referred to as CASE 2. This test can, unfortunately, only prove whether a part of the particle mover is correct. The two-stream instability, hereafter referred to as CASE 3, is one-dimensional in both position as velocity. This instability can occur when a uniform plasma has two equal density Maxwellian beams of speed $\pm \textbf{v}_0$. CASE 3 is the Weibel instability, one-dimensional in space but three-dimensional in velocity. The instability arises in plasmas with anisotropic velocities, meaning the magnitudes of the different axes differ significantly. These test cases must show that energy is conserved throughout the simulation and that the convergence rates for the position and velocity of the particles and the fields are second order. Finally, the phase spaces of the simulations should also reveal structures that are expected for the given instabilities.

%%% ============================================================================================ %%%

\section{Implementation Details}

The developed ECSIM code only depends on the standard \texttt{C++} libraries and the publicly available \texttt{Eigen} library \cite{gael_guennebaud_and_benoit_jacob_and_others_eigen_2010}. The code is parallelised using \texttt{OpenMP}, which supports shared-memory parallelisation and GPU-offloading (from version  4.0 onward). While GPU-offloading is beyond the scope of this work, one could easily adapt this code by replacing the \texttt{Eigen} matrices with a GPU-supported linear algebra library. The code developed within the framework of this project is open-source and can be obtained from \url{https://github.com/BramLeys/ecsim-parareal/tree/main}. The use of \texttt{OpenMP} instead of \texttt{MPI} leads to some implementation choices for parareal. Parareal consists of a serial part, where the coarse integrators and updates are performed, and a parallel section, where the fine solver is computed. This means that there are idle cores during the serial section which can be used for other calculations. When the parallelisation is performed using \texttt{MPI}, pipelining is often used. In this framework, after a time step is updated, it is immediately sent to a different core to be simulated using the fine solver, instead of waiting until the new values for each time step are computed. As we are using \texttt{OpenMP}, we shall instead opt to use these idle cores to improve the accuracy of the coarse solver while using parallelisation to counter the increased computational cost (see Sec. \ref{sub: subcycling} for details). \texttt{OpenMP} is designed for shared-memory systems, and as such, all tests are performed on one node. 
To use multiple nodes, one would have to use \texttt{MPI} to efficiently use the available cores. Since GPUs have a vast amount of computational cores on one node, this would be an option to avoid using multiple nodes. 
The tests were performed on two systems: an MSI GS65 Stealth 9SF with an Intel i7-9750H CPU at a base clock speed of 2.60GHz and 32 GB of RAM and the Tier-2 wICE system at the Flemish Supercomputing Center (VSC). The experiments were run on the Sapphire Rapids nodes, each with 2 Intel Xeon Platinum 8468 CPUs, with a total of 96 cores running at 2.10GHz base clock. Each node has 256 GiB of RAM. The preliminary tests were performed on the MSI GS65, while time-sensitive experiments such as the speedup and performance tests, were performed on the VSC. 

%%% ============================================================================================ %%%
 
\section{Convergence Tests}
\label{sec: convergence}
We start our analysis of the parareal algorithm using the well-known second-order implicit CN integrator, on the one-dimensional diffusion equation:
\begin{equation}
    \frac{\partial u(x,t)}{\partial t} = \frac{\partial^2 u(x, t)}{\partial x}.
\end{equation}
Here, $u$ is the state vector at the position $x$ and time $t$. The spatial domain is discretised into $N_x$ grid cells. The spatial derivative is discretised using the standard second-order centred finite difference scheme, and periodic boundaries are considered on the spatial domain $x \in [0,2\pi[$. 
As a first test, consider a simulation over $1$\, second discretised in $N_t$ time points, with a grid of size $N_x = 100$ where the initial value of each grid point is set to a random value in [-1, 1]. This can seem strange, considering normal tests involve smooth initial conditions. However, these conditions show certain properties that are also observed in the test cases for ECSIM due to the random velocities of the particles. The solution of the discretised system at time step $t_n$ is denoted as $\textbf{U}_n$. 
Figure \ref{fig: CN-convergence-random} shows the serial and parareal simulation errors compared to a reference solution with decreasing time step size. For parareal, this means reducing the time step size of the fine solver while keeping the coarse solver time steps at the original size ($\Delta t_\mathrm{Coarse}= 10^{-2}$). The reference solution, $\tilde{\textbf{U}}_{n}$, is calculated in serial using a time step $50$ times smaller than the finest time step used for the convergence analysis. The error is computed as the relative l2 norm of the difference at $t=1$s:
\[\mathrm{error} = \frac{\|\tilde{\textbf{U}}_{N_t} - \textbf{U}_{N_t}\|_2}{\| \tilde{\textbf{U}}_{N_t} \|_2}\]
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/CNConvergence.eps}
    \caption{Convergence analysis using Crank--Nicolson in serial and in parareal for the diffusion equation using random initial conditions.}
    \label{fig: CN-convergence-random}
\end{figure}
Figure \ref{fig: CN-convergence-random} shows the expected second-order convergence for the errors, although some concerns arise when looking at the internal convergence of the parareal algorithm. 

The parareal algorithm must estimate the error of each time step, $n$, compared to the serial solution during each iteration, $k$, to determine whether the algorithm has achieved the desired accuracy. Our implementation approximates this error by the relative state change between iterations:\[
E^k_n = \frac{\|\textbf{U}^k_n - \textbf{U}^{k-1}_n\|_2}{\|\textbf{U}^k_n\|_2}
\]This thesis will refer to this error as the parareal error from here on. The convergence of the parareal algorithm can thus be investigated by looking at the evolution of these state changes. This reveals that the difference in states between parareal iterations increases between successive iterations instead of converging to 0. This is quite unexpected considering the convergence theorems of parareal \cite{gander_analysis_2007}. CN is an $A_0$-stable method and does not have a CFL condition. One would thus expect the error to decrease in an orderly fashion instead of increasing. 
The change between iterations only reaches 0 when the number of iterations equals the number of time steps. This is expected since, at this point, the fine solver will have propagated throughout the entire time domain. However, when this scenario arises, speedup is impossible to achieve precisely because the fine integrator has been performed once for every time point. 
This behaviour arises from accuracy issues where the used CN method does not have the exact expected order for the coarse solver. The discontinuous initial conditions cause this loss in accuracy, which means the behaviour can be avoided by not considering these conditions. For example, a smooth periodic sine wave as the initial condition ensures a converging trend. However, supposing a situation arises in which such a random initial condition is the desired starting point, it can also be solved by decreasing the time step size of the coarse solver or increasing the grid size. The parareal error for five different situations is plotted in Figure \ref{fig: CN-parareal-convergence}. Note that the tolerance for the parareal iteration is set to $10^{-8}$. The smooth versions show much better convergence. However, the random initial conditions can also converge properly when using the correct discretisation. 
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/CN_parareal_Convergence.eps}
    \caption{Convergence of the errors calculated during parareal applied to the diffusion equation, using different initial conditions and simulation parameters.}
    \label{fig: CN-parareal-convergence}
\end{figure}

The error incurred at the $k^\mathrm{th}$  parareal iteration compared to the fine solution is estimated by the parareal error. Classically, the parareal solution is considered converged if the error for each time step is lower than a given tolerance, $\varepsilon_\mathrm{tol}$. \[
    \max_{n =1...N_t}E^k_n < \varepsilon_\mathrm{tol}
\] 
Since convergence typically starts from the initial condition and propagates forward in time, the condition has been modified so states can converge individually \cite{d_samaddar_parallelization_2010}. This allows the algorithm to skip already converged states, thus reducing computations. In the standard parareal formulation, all states must be updated with each iteration. However, this leads to possibly wasted computations when most states are already converged since differences would not influence the solution much and might also introduce errors induced by resonance. This method makes sense when the selected convergence tolerance is the same as the machine precision. Changes smaller than this tolerance are most likely due to rounding errors. However, this simple reasoning no longer holds when more lenient tolerances are used. It should, therefore, also be investigated whether this modification does not impede the convergence of parareal. Figure \ref{fig: CN-parareal-convergence-tolerance} shows the estimated and actual errors compared to the serial solutions for the two convergence strategies. Only the final iteration is different between the two strategies. However, the error estimate is always an upper bound for the actual error, which is the desired behaviour.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/Tolerance_parareal_check.eps}
    \caption{Convergence of the errors calculated during parareal applied to the diffusion problem using different strategies for the tolerance.}
    \label{fig: CN-parareal-convergence-tolerance}
\end{figure}

We will now demonstrate the correct implementation of the ECSIM algorithm. CASE II is a very simple test case using a uniform plasma distribution where the velocity of nearby particles is defined as the sine of the position, as well as the electric and magnetic fields. This ensures a periodic, smooth initial condition that is used to show the order of the method. It is known that the coupling between the \textbf{particle mover} and \textbf{field solver} can break down the individual order of the two. This is why the results of two test cases, CASE III and CASE IV, will also be compared against the solution of the original paper of Lapenta \cite{lapenta_exactly_2017}. Figure \ref{fig: 1D-1V-convergence} shows the convergence of errors of the three state variables: position, velocity and electric field, for the 1D1V case. The errors are computed against a reference solution using a time step size $50$ times smaller than the smallest size used in the plot. A full second-order convergence is observed; this is due to the absence of interaction with the magnetic field, which is only first-order accurate in time. The magnetic field does not interact with the particles in the 1D1V case due to the Lorentz force, which is used to calculate the acceleration of the particles: 
\[\textbf{F} = q\left(\textbf{E} + \textbf{v} \times \textbf{B}\right)\]
Since the cross-product of two vectors aligned along the same axis is always equal to $\textbf{0}$, this proves that the magnetic field does not influence the particles. Using the same reasoning on the curl equations in Maxwell's equations, we find that the magnetic field is constant, and the electric field dynamics are only influenced by the current. 
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/ECSIM1DConvergence.eps}
    \caption{Convergence of ECSIM on a smooth 1D1V test case.}
    \label{fig: 1D-1V-convergence}
\end{figure}
Table \ref{tab: 1D1V-convergence} shows the values of the convergence rate as a log reduction factor. This factor is calculated as $\frac{\log_{2}\left(\frac{E_i}{E_{i-1}}\right)}{\log_{2}\left(\frac{\Delta t_i}{\Delta t_{i-1}}\right)}$. A second-order convergence would thus be $2$, while first-order accuracy is represented by a $1$ in the table.
\begin{table}[h!]
\centering
\begin{tabular}{cccc}
\toprule
\textbf{Step size decrease} & \textbf{Position}& \textbf{Velocity} & \textbf{Electric Field}\\
\midrule
$0.01 \rightarrow 0.005$ & 1.28  & 0.89 & 1.98 \\
$0.005\rightarrow0.0025$ & 1.98 & 1.81 & 1.98\\
$0.0025\rightarrow0.00125$ & 1.93 & 2.01 & 1.96\\
$0.00125\rightarrow0.000625$ & 1.78& 2.01&1.99\\
\bottomrule
\end{tabular}
\caption{Convergence of ECSIM on the 1D1V smooth problem showing the time step size and log reduction factor.}
\label{tab: 1D1V-convergence}
\end{table}

A 1D3V version is also analysed and shown in Figure \ref{fig: 1D-3V-convergence}. While second-order accuracy can be seen for velocity and the electric and magnetic fields, the order obtained for position is only first-order. This could be due to the initial conditions or the number and shape of the particles. Further testing must prove the correctness of the implementation.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/ECSIM3DConvergence.eps}
    \caption{Convergence of ECSIM on a smooth 1D3V test case.}
    \label{fig: 1D-3V-convergence}
\end{figure}
Table \ref{tab: 1D3V-convergence} shows the log reduction factors for the smooth 1D3V test case. As stated, the position only achieves first-order accuracy, while the other state variables are second-order accurate.
\begin{table}[h!]
\centering
\begin{tabular}{ccccc}
\toprule
\textbf{Step size decrease} & \textbf{Position}& \textbf{Velocity} & \textbf{Electric Field} & \textbf{Magnetic Field}\\
\midrule
$0.01 \rightarrow 0.005$ & 1.43  & 1.96 & 1.65&1.83 \\
$0.005\rightarrow0.0025$ & 1.11 & 2.11 & 1.87&1.96\\
$0.0025\rightarrow0.00125$ & 1.02 & 2.03 & 1.96&1.99\\
$0.00125\rightarrow0.000625$ & 1.01& 2.00&1.97&1.99\\
\bottomrule
\end{tabular}
\caption{Convergence of ECSIM on the 1D3V smooth problem showing the time step size and log reduction factor.}
\label{tab: 1D3V-convergence}
\end{table}
 
 
We turn to two practical test cases to further investigate the correctness of the code. CASE I is an electrostatic two-stream instability. It consists of a uniform plasma divided into two Maxwellian beams with a random thermal speed added to their opposite base speeds. We consider this system with $10000$ particles moving in the periodic domain $x \in [0, 2\pi[$ divided into $128$ grid cells. 
 Figure \ref{fig: 1D-1V-sim} shows the velocity distribution of the particles in 2D phase space for both the initial condition (top) and the state after $50$\,seconds (centre). The energy conservation (bottom) is also shown as a function of time for the two-stream instability. The phase space shows the creation of zones without electrons, as in \cite{lapenta_advances_2023}. This is a known effect of the two-stream instability and, in combination with the energy conservation up to machine precision, leads us to accept the correctness of the 1D1V implementation.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/Sim_plots_1D.eps}
    \caption{Phase space at the initial condition (top), after $50$ seconds (middle) and the error in energy (bottom) of the two-stream instability simulated using serial ECSIM.}
    \label{fig: 1D-1V-sim}
\end{figure}

For a test case with both magnetic and electric influence, we turn to CASE IV. While it is still 1-dimensional in space, it is three-dimensional in velocity and consequently keeps track of the electric and magnetic fields in three dimensions. Figure \ref{fig: 1D-3V-sim} shows the phase space and energy conservation of CASE IV. As in \cite{lapenta_advances_2023}, islands are seen in the phase space, as well as an error in energy that hovers around machine precision. These observations allow us to assume the implementation is correct. On top of these tests, the results on precalculated initial conditions were also compared between the implemented \texttt{C++} code and the available \textbf{Matlab} code of \cite{lapenta_advances_2023}.
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/Sim_plots_3D.eps}
    \caption{Phase space at the initial condition (top), after $62$ seconds (middle) and the error in energy (bottom) of the transverse stream instability simulated using serial ECSIM, blue: positive $\textbf{v}_y$, red: negative $\textbf{v}_y$.}
    \label{fig: 1D-3V-sim}
\end{figure}
We now use the ECSIM method for both the fine and coarse integrator in parareal to simulate CASE IV. The coarse solver uses a time step size of $10^{-2}$, while the step size of the fine solver is reduced further and further. This analysis gives the same convergence plot as the serial case as long as the tolerance used in parareal is more accurate than the lowest achieved accuracy for any of the state variables. Similar to CASE I, the required number of parareal iterations and parareal errors are not encouraging. The error rises before going down, and the number of parareal iterations equals the number of time steps. As before, one can either coarsen the grid discretisation or use smaller time step sizes for the coarse solver to solve this issue. This might be connected to the CFL condition associated with ECSIM, although the issues arise with a higher $\frac{\Delta t}{\Delta x}$ factor than for the linear case. 
Figure \ref{fig: 1D-3V-parareal-convergence} shows the convergence of parareal for simulating CASE IV using 10000 particles, a fine time step size of $10^{-4}$ and a coarse step of $10^{-2}$. 
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/3D_parareal_Convergence.eps}
    \caption{Convergence of errors calculated during parareal applied to the transverse stream problem using a grid discretisation of 512 and 5000 cells.}
    \label{fig: 1D-3V-parareal-convergence}
\end{figure}
These time step sizes limit the effective time range that is calculated. One might thus wonder whether the results of parareal are trustworthy after longer periods. Especially considering the test cases under investigation are instabilities showing highly non-linear behaviour. parareal, however, has even been applied with success to chaotic systems \cite{d_samaddar_parallelization_2010}. For these chaotic systems, the individual solutions reached differed due to the predictor-corrector-like nature of the parareal algorithm. The statistical properties of these solutions, however, did get correctly simulated.
It is worth noting that when parareal checks for convergence, it estimates the error compared to the fine solution as the relative change in the state variables over two consecutive iterations. This means that for a converging algorithm, one would hope the error compared to the fine solution to be bounded by the state change. This can be seen in Figure \ref{fig: error-vs-statechange} where the maximal error compared to the fine solution and the maximal parareal error are shown per iteration. 
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/est_vs_act_err_parareal_check.eps}
    \caption{Error compared to the fine solution and parareal error during each parareal iteration of a simulation of the transverse stream instability.}
    \label{fig: error-vs-statechange}
\end{figure}

While exact energy conservation is not a goal of the current work, it should be noted that for these test problems, it was not necessary to use a tolerance equal to the machine precision for parareal to obtain energy conservation up to machine precision. This manifests itself in the other experiments, where the error incurred in energy is significantly smaller than the estimated error for the state variables. This is shown in figure \ref{fig: energy-vs-statechange}, where the energy conservation of the solution is plotted for several iterations alongside the max state changes. This indicates that the parareal error also gives a generous bound for the energy error.

These results, supplemented by the fact that for practical simulations, one often does not desire the fine solution to be accurate up to machine precision, lead to the practical observation that one can often get very good (almost exact) energy conservation while using more lenient tolerances, e.g., $10^{-10}$ or $10^{-12}$. This is especially useful, as it is often not desirable to approximate the fine solution up to machine precision since the fine solver itself is often only a discretised approximation of the actual solution.
 \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/est_vs_energy_err_parareal_check.eps}
    \caption{Error in energy and the parareal error during each parareal iteration of a simulation of the transverse stream instability}
    \label{fig: energy-vs-statechange}
\end{figure}

%%% ============================================================================================ %%%

\section{Temporal Coarsening}

The choice of fine and coarse integrators for parareal can massively influence the algorithm's performance. It is thus crucial to decide on a good combination. It is well-known that using smaller time step sizes in ODE solvers results in more accurate solutions than larger sizes. Using the same integrator method for both the coarse and fine solver while employing a large time step size for the coarse solver and a smaller step size for the fine solver thus constitutes a natural choice for a parareal implementation. The coarse solver is expected to yield a ``rougher'' solution estimate at every coarse time point compared to the fine solver. Suppose $r$ fine time steps are computed for each coarse time step, using an ODE solver of order $l$. In this case, the error incurred by the fine integrator is expected to be a factor $\mathcal{O}(n^r)$ smaller than the coarse approximation. 
This choice of fine and coarse solver also allows for easy comparisons in terms of computational complexity between the fine and coarse. If the fine solver performs $100$ steps for each coarse step, one would expect that the fine integrator is $100$ times slower than the coarse solver.

To define the fine and coarse time step sizes, we fix one and calculate the other based on the chosen ratio $r$. We now show how this factor influences the performance of parareal by plotting the speedup for increasing $r$.

The first experiment chooses the coarse step size as a constant and reduces the fine size. Although this allows us to show speedup results approaching the theoretical limit from \ref{eq: speedup}, it is practically less valuable. In a typical use case, a certain accuracy is desired over the simulation time period. Since parareal will approach the accuracy of the fine solver, the fine integrator should reach the desired accuracy. This means the fine time step size is connected to the desired accuracy, and as such, it does not make sense to go to much smaller step sizes, even if the speedup would be better. Decreasing the fine time step size still increases the computational runtime of the parareal algorithm, the higher speedup only indicates that it rises less steeply than the runtime of the serial solution. Therefore, it is also informative to show how the speedup and computational runtimes change when the fine time step size is fixed. This plot can be used to choose the best coarse integrator step size for a given fine.

Figure \ref{fig: temporal-coarsening} shows the computational runtime (left) and speedup (right) obtained when choosing either a fixed coarse time step size (top) or fine step size (bottom). All experiments use 96 cores of one node on the VSC and have 512 grid cells for spatial discretisation. The top graphs use a fixed coarse time step size, $\Delta t_\mathrm{Coarse} = 10^{-3}$, while the bottom plots use a constant fine step size $\Delta t_\mathrm{Fine} = 10^{-5}$. 

We first look towards the top plots. As stated before, the computational runtime increases as the factor between fine and coarse increases. Confirming that needlessly increasing the accuracy of the fine solver is not advised. Note that the scales on the serial and parareal solver axes differ, where the serial solve is more expensive than the parareal solution. 
The top right graph shows that making the fine solver more expensive helps attain higher speedup. This means it is beneficial to perform parareal when a very fine discretisation is desired. This upward motion is only warranted by the assumption that the number of needed parareal iterations remains the same, which is always $3$ in the tested results (except when $\frac{\Delta t_\mathrm{Coarse}}{\Delta t_\mathrm{Fine}}=1$). This is shown by the theoretical bound shown in the figure, which approximates the speedup equation \ref{eq: speedup} with the ratio between the used number of cores to the number of required parareal iterations, $\frac{p}{k} = 48$. The required number of iterations likely remains the same due to the coarse timestep already being very small to account for the CFL conditions of the ECSIM algorithm. This allows parareal to converge quickly. Note that the speedup graph seems to approach the theoretical bound as $\frac{\Delta t_\mathrm{Coarse}}{\Delta t_\mathrm{Fine}}$ increases, validating the conclusions of equation \ref{eq: speedup}. A speedup of $27$ is achieved for a factor of $512$.

The bottom graphs of Figure \ref{fig: temporal-coarsening} show the results of keeping the fine time step size constant while increasing the step size of the coarse integrator. To obtain comparable results for the speedup of the test with constant fine solver step size, the time frame was always chosen to be equal to $96\cdot\Delta t_\mathrm{Coarse}$. This explains why the computational runtime of the serial solver increases in the bottom left plot. This measure ensures the parallel section can always appoint exactly one core per coarse time step, putting more emphasis on the parallel aspect of the parareal algorithm. In the bottom right figure, it can be seen that, as the number of coarse time steps increases, the speedup obtained increases to a certain point before it plummets. This may be explained as follows: choosing a very cheap solver reduces serial computational time, thereby speeding up the simulations. However, the cheap coarse solver may not yield reasonably accurate solutions, necessitating additional parareal iterations that can negate the computational gains obtained from choosing a cheap coarse solve. The theoretical bound shows that the number of iterations needed by parareal decreases as the coarse and fine time step sizes get closer. We obtain the largest speedup in this scenario for a $\Delta t_G/\Delta t_F = 128$. 
Here, the approximation of the coarse solver is accurate enough for the parareal algorithm to converge within a reasonable number of iterations without incurring significant overhead. Further reductions in the coarse time step size to get more accurate coarse solutions and, consequently, reduce the number of parareal iterations prove to be counter-productive. Although they decrease the number of iterations, it is not a large enough difference.

 \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/time_step_constant_coarse.eps}
    \includegraphics[width=1\linewidth]{figures/eps/time_step_constant_fine.eps}
    \caption{Computational runtime and speedup of parareal using temporal coarsening during the simulation of the transverse stream instability.}
    \label{fig: temporal-coarsening}
\end{figure}
For the fixed $\Delta t_\mathrm{Coarse}$, we expect that the increase of the speedup along with the factor $\frac{\Delta t_\mathrm{Coarse}}{\Delta t_\mathrm{Fine}}$ also occurs for different starting $\Delta t_\mathrm{Coarse}$. This behaviour is shown in Figure \ref{fig: temporal-coarsening-speedup_coarse_const}. The increase can be seen for all cases, although for $\Delta t_\mathrm{Coarse} = 10^{-2}$, it is damped. This case is quite close to the CFL condition of the PIC method and thus has a large amount of parareal iterations where the parareal error is not properly converging. As stated before, the time periods which are simulated are dependent on the coarse time step size, so one should not use this plot to choose between $\Delta t_\mathrm{Coarse}$ when a specific time range is desired.
 \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/time_step_constant_coarse_speedup.eps}
    \caption{Speedup of parareal with temporal coarsening using different constant $\Delta t_\mathrm{Coarse}$.}
    \label{fig: temporal-coarsening-speedup_coarse_const}
\end{figure}
We also expect to see the peak for different $\Delta t_\mathrm{Fine}$. This is shown in Figure \ref{fig: temporal-coarsening-speedup_fine_const}. The peak for $\Delta t_\mathrm{Fine} =10^{-4}$ occurs at $\frac{\Delta t_\mathrm{Coarse}}{\Delta t_\mathrm{Fine}} = 32$, while the highest speedup for $\Delta t_\mathrm{Fine} = 10^{-6}$ is expected to appear at a ratio beyond $512$. The reason why the turning point moves further along the x-axis is because the fine time step size is smaller. Thus, the ratio of the fine and coarse sizes can be larger before the coarse integrator becomes too inaccurate.
 \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/time_step_constant_fine_speedup.eps}
    \caption{Speedup for parareal with temporal coarsening using different constant $\Delta t_\mathrm{Fine}$.}
    \label{fig: temporal-coarsening-speedup_fine_const}
\end{figure}

\subsubsection{Subcycling}
\label{sub: subcycling}

The standard particle-in-cell algorithm consists of four steps, performed cyclically. Firstly, the particles move during the \textbf{particle mover}, after which they are collected and projected onto the grid to compute the current and charge density. These currents and charge densities at each grid cell are then used during the \textbf{field solver} section to calculate the change in electric and magnetic fields. These new forces are then projected onto the particles, whose movement will be influenced in the next \textbf{particle mover} step. If we reduce or increase the time step size, this does not influence this cycle. There is, however, an adaptation of the algorithm that does change the underlying iteration scheme. The modification in question is called subcycling and consists of performing the \textbf{particle mover} multiple times before moving on to the \textbf{field solver}. In this setting, the time step is subdivided into $\nu$ (not necessarily equal sized) substeps, $\Delta t_\nu$. The particles then move $\nu$ times influenced by the same constant fields, after which the (weighted) average of the positions and velocities are used to update the fields. These fields are then projected onto the particles, and the cycle starts anew. Subcycling can reduce computational costs in situations where the dynamics of the particles are much faster than those of the fields. This allows for the time step size, $\Delta t$, to be chosen based on the dynamics of the slower fields. In contrast, the substep sizes $\Delta t_\nu$ are chosen to be small enough to accurately approximate the movement of the particles. Often, particles move in cyclotron orbits; in situations like these, it is also possible to use subcycling for gyro-averaging. This is done by selecting $\Delta t$ to step over the gyration time scale while using the $\Delta t_\nu$ to average the movement during the gyromotion.

We now turn our attention to using subcycling for the coarse solver in parareal. Subcycling can be used during the coarse integrator to obtain a more accurate approximation of the fine solution without incurring the high costs of reducing the coarse time step size, which would require performing more steps. A second approximation is made during the implemented subcycled ECSIM code; the velocity is kept constant across the subcycles\cite{lapenta_advances_2023}. This allows us to calculate all of the subcycles in parallel. This parallelisation can use the cores that are not used during the serial calculations of parareal. Since each subcycle can be calculated in (almost) perfect parallelism, the expected cost of subcycling is negligible under the constraint that the number of subcycles remains lower than or equal to the number of cores available. Our ECSIM code implements subcycling by injecting $\nu-1$ extra equispaced time points in each time step, at which only the position is updated. These positions are then averaged during the \textbf{field solver} to obtain a more accurate solution. This hopefully decreases the number of iterations parareal needs to converge. The fine solver uses a time step size of $10^{-5}$, while the coarse step size is equal to $10^{-3}$.

It can be seen in Figure \ref{fig: temporal-subcycling-errors} that the accuracy does indeed increase for more subcycles. For example the version with $10$ subcycles has a parareal error of $2.8\cdot 10^{-8}$ at iteration $9$, while the simulation without subcycles has an error of $5.4\cdot 10^{-8}$.  The increased accuracy can also be noticed in the number of time steps that converge after each parareal iteration, which increase along with the number of subcycles. Figure \ref{fig: temporal-subcycling}, however, shows that one fails to obtain any improvement in the computational cost incurred. This is due to parallelisation overhead and the need to average the positions of the particles after the subcycles. These experiments converge quickly, only needing $3$ iterations to converge. The difference in how many time steps have converged at an iteration can be quite different, however. For example, for the unsubcycled version, 16 steps have converged after the second iteration, while the version with $\nu = 10$ already has 75 converged time steps. The decreased parareal errors and increased number of time steps converging per parareal iteration show that increased performance might be attainable in certain scenarios. Example situations would involve many time steps and exhibit relatively poor convergence for the parareal algorithm, requiring numerous iterations.

  \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/subcycle_speedup.eps}
    \caption{Speedup of the parareal simulation compared to a serial fine solve, using different amounts of subcycling.}
    \label{fig: temporal-subcycling}
\end{figure}

  \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/subcycle_parareal_convergence.eps}
    \caption{Estimated errors during parareal iterations for different amounts of subcycling.}
    \label{fig: temporal-subcycling-errors}
\end{figure}

\section{Core Scaling}
In chapter \ref{cha: pint}, two strategies are discussed to simulate long time frames; [A] each processor gets multiple coarse steps assigned to it, or [B] the time domain is split into separate chunks on which parareal is performed in serial. These can be used when the number of available cores is insufficient to assign a separate processor to each coarse time step. 
We will now discuss a core aspect of any parallel algorithm: the scalability with respect to the number of cores used. This information is crucial in deciding which strategy is most fitting for the desired simulation time period. This section assumes that the number of time steps equals the number of cores. Figure \ref{fig: core scaling} demonstrates how the speedup (left) and parallel efficiency (right) change depending on the number of time steps. The speedup steadily rises with the increase in cores, however, the parallel efficiency shows that this increase is not commensurate with the extra cores used. It insinuates that it can be more efficient to split up the time domain into multiple sequential parareal sections when long-duration simulations are desired. To remedy this, one could consider larger time step sizes for the coarse solver to more quickly cross large time spans. Special care should be taken, that the convergence of parareal is not impeded due to getting too close to the CFL condition. The decrease in parallel efficiency might necessitate the use of multiple computing nodes using \texttt{MPI} or offloading to GPU. This would constitute a subject for future work. Again, the theoretical bound is shown, where the parallel efficiency is approximated with $\frac{1}{k}$. Keeping this in mind, we can see that the number of iterations needed for larger timeframes only increases once from $2$ to $3$. This indicates that the reason the parallel efficiency is not constant, is not due to an increase in iterations. It is most likely caused by parallelisation overhead.
  \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/core_scaling_test.eps}
    \caption{Speedup (left) and parallel efficiency (right) of the parareal algorithm for an increasing number of cores, where each core is assigned to one coarse time step.}
    \label{fig: core scaling}
\end{figure}

%%% ============================================================================================ %%%

\section{Linear Solvers}

Here, we investigate the performance of some widely used (direct and indirect) solvers to approximate the solution to the linear system of equations obtained by discretising Maxwell's equations. Direct solvers have the benefit of computing the solution \textit{exactly}; however, they may be computationally prohibitive for large systems. In a serial context, the solution should be accurate up to machine precision due to the desired exact energy conservation. Since the basic parareal algorithm does not conserve this property \cite{gander_analysis_2014}, it would have to iterate to highly stringent tolerances to achieve machine precision accuracy. Since total energy conservation is not a requirement for this work, more options are available to solve this linear system. The matrix resulting from the linear system is sparse and has a block-diagonal structure, which lends itself to iterative solvers. 
Nevertheless, we also consider the LU decomposition method along with the commonly used iterative solvers, GMRES and BiCGStab \cite{youcef_saad_martin_h_schultz_gmres_1986,van_der_vorst_bi-cgstab_1992}. We investigate the effect of choosing different combinations of these three, for the coarse and fine solvers. It is known that the computational cost of an iterative solver depends on the time step size \cite{einkemmer_adaptive_2018, hochbruck_exponential_1998}, whereas the cost of a direct solver is independent of it. The cost of an iterative solver also depends on the initial guess. This insinuates that if one chooses the previous solution as an initial guess, a large time step for the coarse solver would make an LU decomposition more favourable than an iterative solver. It would, after all, be expected that the large timestep induces a greater difference between the previous and next solution. However, for the fine solver that considers multiple small time steps, an iterative solver may only need a few iterations to converge and, consequently, be faster than the LU method. To test this hypothesis, different permutations of the linear solvers are used for the fine and coarse integrators. Special care is also taken to choose a proper value for the user-defined tolerance for the parareal algorithm and the iterative solver under consideration. Finally, different preconditioners are employed to speed up the convergence of the iterative solvers.

Unfortunately, the discretisation matrix is not symmetric, invalidating specific solvers, such as Cholesky-based decompositions. The underlying matrix's diagonal dominance makes it suitable for iterative solvers, such as the generalised minimal residual method. In these experiments, the problem statement will not change between runs; only the solvers will be different; thus, any decrease in runtime indicates a positive result.

\subsection{Sparse Lower Upper Decomposition}
Lower Upper Decomposition factorises a matrix $\textbf{A}$ into a lower triangular matrix $\textbf{L}$ and upper triangular $\textbf{U}$ such that $\textbf{A} = \textbf{LU}$. To ensure a proper decomposition for any square matrix, it might be necessary to perform a permutation of rows so that there are no 0 elements on the diagonal of $\textbf{A}$ at each step. This is called LU decomposition with partial pivoting. This leads to $\textbf{PA} = \textbf{LU}$; in the sparse case, this permutation matrix $\textbf{P}$ is also chosen to minimise fill-in. Fill-in is the occurrence of a non-zero in the $\textbf{L}$ or $\textbf{U}$ matrices, while the element at the same place in the $\textbf{A}$ matrix is zero. The factorisation matrices can cheaply compute the solution to the linear system using forward and backward substitution.
\begin{align}
    \textbf{Ax} &= \textbf{b}\\
    \textbf{LUx}&= \textbf{b}\\
    \textbf{Ly} = \textbf{b} \quad &\quad \textbf{Ux} = \textbf{y}
\end{align}

\subsection{Generalised Minimal Residual Method}
GMRES is an iterative linear solver based on Krylov subspaces, starting from a given initial guess $\textbf{x}_0$. The n-th Krylov subspace can be written as $K_n = \mathrm{span}(\{\textbf{A}\textbf{x}_0-\textbf{b}, \textbf{A}(\textbf{A}\textbf{x}_0-\textbf{b}),...,\textbf{A}^{n-1}(\textbf{A}\textbf{x}_0-\textbf{b})\})$. The solution $\textbf{x}$ is approximated at iteration n by $\textbf{x}_n = \textbf{x}_0 + \textbf{Q}_n \textbf{y}_n$, where $\textbf{Q}_n$ is an orthogonal basis for $K_n$ and $\textbf{y}_n$ are the coefficients that minimise the error $\|\textbf{x}-\textbf{x}_n\|$. GMRES can suffer from storage and computational issues as the dimension of the Krylov subspace increases. \textbf{GMRES} must store the Hessenberg matrix, which contains all the orthogonal vectors forming the orthogonal basis. Additionally, it has to orthogonalize each new vector against all previously computed vectors, which can become quite costly. This can be remedied by ``restarting'' the algorithm. This involves starting a new GMRES algorithm using the previously found solution as the initial guess. The method is often used to solve large sparse systems and can be applied to any nonsingular square matrix\cite{he_parallel_2023}.


\subsection{Biconjugate Gradient Stabilized Method}
The biconjugate gradient stabilized method, as the name suggests, is based on the biconjugate gradients method, which in turn is a generalisation of the conjugate gradients method. Like the Cholesky decomposition-based methods, the conjugate gradient method is not applicable to non-symmetric matrices. The biconjugate gradient method can solve non-symmetric systems; however, it is numerically unstable, leading to the use of the \textbf{BiCGSTAB} in most practical applications. The \textbf{BiCGSTAB} algorithm performs two BiCG steps followed by a stabilization step. BiCG uses a biorthogonal system, needing to keep track of two separate residuals, which can lead to unstable behaviour. The BiCGSTAB method uses both a direction and a magnitude parameter, defined by minimizing the resulting residuals like GMRES, to smooth convergence and stabilize the method. It is also often used for large sparse matrices \cite{yang_improved_2002,krasnopolsky_revisiting_2020} and might converge faster than GMRES. It does not have the option of restarting the algorithm, although the memory requirements should be smaller than for GMRES. 

The use of an iterative solver necessitates a user-specified tolerance to define convergence for the solution of the linear system. One of the findings of this work indicates that this choice cannot be made without considering the tolerance chosen for parareal. Our findings suggest that a tolerance must be chosen at least multiple orders of magnitude lower than the one used for parareal. Otherwise, the convergence of parareal is inhibited, and the error estimation during parareal is no longer trustworthy. This is demonstrated in Table \ref{tab: tolerance_lin_solver_8}, where the required amount of parareal iterations is shown for different ratios of tolerances while using GMRES. When the tolerances for parareal and the linear solver are both $10^{-8}$, the number of parareal iterations is much higher than when the tolerance for the linear solver is more stringent. The errors incurred compared to the serial solution also show concerning behaviour for a tolerance of $10^{-8}$ and $10^{-9}$. The parareal error is consistently lower than the error with regards to the serial solution, invalidating its use as an upper bound. Only when a tolerance of $10^{-12}$ is chosen for the linear solvers, does the parareal error become trustworthy again. Note that the error compared to the serial solution is already below the requested tolerance for $10^{-10}$ and $10^{-11}$. However, since the parareal error is higher than the error with regareds to the serial solution, this is not always guaranteed and should thus be avoided.
\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Tolerance}& $10^{-8}$ & $10^{-9}$ & $10^{-10}$ & $10^{-11}$& $10^{-12}$\\
        \hline
         Parareal iterations ($k$)& 6 & 2 & 2 & 2 & 2\\
         Parareal error & $4.0\cdot10^{-9}$ & $1.1\cdot10^{-9}$ & $2.0\cdot10^{-10}$ & $1.6\cdot10^{-10}$ & $1.6\cdot 10^{-10}$\\
         Error (w.r.t serial) & $2.8\cdot10^{-7}$ & $3.2\cdot10^{-8}$ & $4.4\cdot10^{-9}$ & $4.0\cdot10^{-10}$& $4.2\cdot 10^{-11}$\\
        \hline
    \end{tabular}
    \caption{Results of parareal on CASE I using different tolerances for the linear solver with a tolerance of $10^{-8}$ for parareal.}
    \label{tab: tolerance_lin_solver_8}
\end{table}
These results are confirmed in Table \ref{tab: tolerance_lin_solver_9} where the same tolerances are used for the linear solver, but the parareal tolerance is set to $10^{-9}$. The same conclusions can be drawn, although a specific point of interest is the use of a lower tolerance for the linear solver than for parareal. In this case, the needed number of parareal iterations equals the number of time steps, but the error incurred by the parareal solution is lower than the requested tolerance. This situation is not useful in a speedup sense, however, and we also assume that this error is not guaranteed due to the higher tolerance for the linear solver.
\begin{table}[htbp]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Tolerance}& $10^{-8}$ & $10^{-9}$ & $10^{-10}$ & $10^{-11}$& $10^{-12}$\\
        \hline
         Parareal iterations ($k$) & 12 & 5 & 2 & 2 & 2\\
         Parareal error & $0.0$ & $6.5\cdot10^{-10}$ & $2.0\cdot10^{-10}$ & $1.6\cdot 10^{-10}$& $1.6\cdot 10^{-10}$\\
         Error (w.r.t serial) & $7.7\cdot 10^{-10}$ & $2.4\cdot 10^{-8}$ & $4.4\cdot 10^{-9}$ & $4.0\cdot 10^{-10}$ & $4.2\cdot 10^{-11}$\\
        \hline
    \end{tabular}
    \caption{Results of parareal on CASE I using different tolerances for the linear solver with a tolerance of $10^{-9}$ for parareal.}
    \label{tab: tolerance_lin_solver_9}
\end{table}

We now consider the use of different linear solvers. This could allow for better speedup or computational runtime. A possible strategy is to use a very accurate but slow direct LU solver for the fine solution for high accuracy while using a less accurate iterative solver in the coarse. However, a coarse timestep could lead to more iterations needed for the iterative solvers to converge to the desired accuracy, thus increasing the overall computational time. Figure \ref{fig: linear solver combination} shows the results of the same simulation using each linear solver combination. The left panel shows the speedup achieved, and the right panel illustrates the computational runtime for each combination under consideration. Each cluster of three bars indicates the use of a different fine solver. Each blue bar indicates a simulation where the coarse solver uses an LU solver, the orange bars represent GMRES solvers, and the green corresponds to BiCGSTAB. While both bar plots are important, the right plot is essential in understanding the influence of the different solvers. The bar plot on the left clearly shows a preference for a costly LU solver for the fine integrator. However, this may be misleading as one has to consider the overall computational runtime. The figure on the right shows that it is best to use iterative solvers for both the fine and coarse solvers, and there is no significant difference between GMRES and BiCGSTAB. This discrepancy in interpretation is caused by the difference in computational runtime caused by the multiple fine solvers. 
As expected of parareal, the solutions are all equally accurate to their respective serial solutions. 
  \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/solver_test.eps}
    \caption{Left: Speedup for parareal simulation using different combinations of linear solvers for coarse and fine. Right: Time needed for a parareal simulation using different combinations of linear solvers for coarse and fine.}
    \label{fig: linear solver combination}
\end{figure}

\subsection{Preconditioners}
Preconditioners are transformations performed on the linear system to improve the convergence rate of linear solvers. Here, we define preconditioners to be the inverse of a matrix $\textbf{P}$ defined so that $\textbf{P}^{-1}\textbf{A}$ has an increased convergence rate for iterative methods that solve $\textbf{Ax} = \textbf{b}$. This means a preconditioned system will solve $\textbf{P}^{-1}\textbf{Ax} = \textbf{P}^{-1}\textbf{b}$. Three different types of preconditioners will be tested. The first preconditioner is the identity matrix, i.e., no preconditioning. The second is the Jacobi preconditioner, for which the preconditioning matrix corresponds to the diagonal of the matrix $\textbf{A}$; it should perform well for diagonally dominant matrices. The final preconditioner that is examined is the incomplete LU factorisation. The general idea of incomplete LU is to factorise $\textbf{A}$ into the product of a lower and upper triangular matrix such that $\textbf{A} \approx \textbf{LU}$, where $\textbf{L}$ and $\textbf{U}$ share a sparsity pattern with $\textbf{A}$. This reduces memory and computational cost requirements whilst providing a rough estimate of the solution. One can then use this solution as a starting point.

Figure \ref{fig: preconditioners} shows the performance of the three preconditioners under consideration for different time step sizes for a serial implementation of CASE IV. While there is no significant difference between the identity and Jacobi preconditioners, one can see that the incomplete LU decomposition is faster when the time step size is larger and slower than the other preconditioners when the step size is reduced. While the incomplete LU preconditioner is more expensive to calculate, the iterative solver only needs a small number of iterations afterwards to solve the system during the test. This contrasts the identity and Jacobi preconditioners, which are fast to perform but might not help the solver converge fast enough, leading to many iterations when the initial guess is inaccurate.
 \begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figures/eps/precond_test.eps}
    \caption{Time needed for serial solve of CASE IV using different preconditioners for GMRES.}
    \label{fig: preconditioners}
\end{figure}

%%% ============================================================================================ %%%

% \section{Discussion}

% \pjd{Suggestion: remove this section. If you think something's really important, include it in the previous (sub)sections. Non-smooth initial data does not require detailed analysis. Generally, you never start with non smooth data. Turbulence or shocks develop in simulations as result of the initial/boundary conditions and other physical effects.}

% While the previous section showed the results of the different experiments, they will be linked together in this section. There are, after all, interesting interplays between the results that must not be overlooked. For a first example of the interconnection, one must look at the behaviour of Parareal for the non-smooth initial conditions outlined in \ref{sec: convergence}. It is clear that the convergence of Parareal strongly depends on the smoothness of the solution; in the case of non-smoothness, Parareal requires an accurate coarse solver, or it will not converge in an orderly fashion, sometimes even diverging. As a result one would prefer to use Parareal on smooth problems. If this is not the case, a more accurate coarse solver is needed, thus lowering the possible speedup. This, however, already determines the mode in which the linear solver will have to operate, more likely in the region where iterative solvers with quick preconditioners are best. The required small timesteps for the coarse solver also mean that many steps would be needed to cover more extended periods, which was found to not help the speedup of the algorithm. This means a more relaxed "CFL"-esque condition would significantly increase the number of possible implementations and use cases. \color{red} This this effect might be caused by particles moving so fast between time points that cells never experience their presence in the coarse solver. In this case, it might be possible to decrease the effect by increasing the b-splines' order in the particle-in-cell method so that a single particle influences more grid cells. Also, check whether the "CFL" condition is worse for parareal or the same as the linear.\color{black}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 